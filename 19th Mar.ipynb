{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb06c53-e2f8-41fd-84d5-92addb282dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "Ans:- * Min-Max Scaling :\n",
    "\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique that linearly scales features (attributes) \n",
    "of a dataset into a specific range, typically between 0 and 1. This process brings features with different scales and units \n",
    "onto a common ground, improving the performance of subsequent machine learning algorithms, especially distance-based ones, \n",
    "which are sensitive to feature magnitudes.\n",
    "\n",
    "> How it Works:\n",
    "\n",
    "Calculate Minimum and Maximum: For each feature, determine the minimum and maximum values across all samples in the training\n",
    "data.\n",
    "\n",
    "Apply Linear Transformation: For each sample, subtract the minimum value from its feature values and then divide by the \n",
    "difference between the maximum and minimum values:\n",
    "\n",
    "scaled_value = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "where x is the original value, min(x) is the minimum value for that feature, and max(x) is the maximum value for that feature.\n",
    "\n",
    "This transformation maps the original values to the specified range (usually 0-1).\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a dataset with two features:\n",
    "\n",
    "Sample\tFeature A\tFeature B\n",
    "1\t     10\t             20\n",
    "2\t     20\t             30\n",
    "3\t     30\t             40\n",
    "Feature A:\n",
    "Minimum: 10\n",
    "Maximum: 30\n",
    "Scaled values:\n",
    "Sample 1: (10 - 10) / (30 - 10) = 0\n",
    "Sample 2: (20 - 10) / (30 - 10) = 0.333\n",
    "Sample 3: (30 - 10) / (30 - 10) = 1\n",
    "Feature B:\n",
    "Minimum: 20\n",
    "Maximum: 40\n",
    "Scaled values:\n",
    "Sample 1: (20 - 20) / (40 - 20) = 0\n",
    "Sample 2: (30 - 20) / (40 - 20) = 0.5\n",
    "Sample 3: (40 - 20) / (40 - 20) = 1\n",
    "Now both features have values between 0 and 1, making them more comparable for machine learning algorithms.\n",
    "\n",
    "Advantages of Min-Max Scaling:\n",
    "\n",
    "Simple and easy to implement.\n",
    "Improves convergence in gradient-based algorithms.\n",
    "Makes features comparable in distance-based algorithms.\n",
    "Disadvantages of Min-Max Scaling:\n",
    "\n",
    "Sensitive to outliers, which can shift the range and distort other values.\n",
    "Not suitable for features with negative values if the desired range is 0-1.\n",
    "Alternatives to Min-Max Scaling:\n",
    "\n",
    "Standard scaling (z-score normalization): Centers data around a mean of 0 and a standard deviation of 1. Less sensitive to \n",
    "outliers but may not be suitable for non-normally distributed data.\n",
    "Robust scaling: Similar to standard scaling but uses median and MAD (Median Absolute Deviation) instead of mean and standard\n",
    "deviation, making it more resilient to outliers.\n",
    "When to Use Min-Max Scaling:\n",
    "\n",
    "Features have different scales and units.\n",
    "Distance-based algorithms are used.\n",
    "Outliers are not a major concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61447157-6212-4307-996d-ac69dc792473",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "ANs:- \n",
    "\n",
    "> Unit Vector\n",
    "\n",
    "Scales each data point to have a magnitude (length) of 1.\n",
    "Preserves the direction of the original data points.\n",
    "Useful for algorithms that rely on distances between data points, such as k-nearest neighbors or cosine similarity.\n",
    "Min-Max Scaling\n",
    "\n",
    "Scales each feature (column) to a specific range, typically between 0 and 1.\n",
    "Does not preserve the direction of the original data points.\n",
    "Useful for algorithms that are sensitive to the magnitudes of features, such as gradient descent-based algorithms.\n",
    "Here's an example to illustrate the differences:\n",
    "\n",
    "Consider the following dataset:\n",
    "\n",
    "Feature A\tFeature B\n",
    "10\t20\n",
    "20\t30\n",
    "30\t40\n",
    "Unit Vector Scaling:\n",
    "\n",
    "Python\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[10, 20], [20, 30], [30, 40]])\n",
    "\n",
    "def unit_vector(data):\n",
    "    return data / np.linalg.norm(data, axis=1)[:, np.newaxis]\n",
    "\n",
    "scaled_data_unit_vector = unit_vector(data)\n",
    "\n",
    "print(scaled_data_unit_vector)\n",
    "Use code with caution. Learn more\n",
    "Output:\n",
    "\n",
    "[[ 0.4472136  0.89442719]\n",
    " [ 0.5547002  0.83205029]\n",
    " [ 0.6        0.8       ]]\n",
    "As you can see, each data point now has a magnitude of 1, while the relative distances between the points are preserved.\n",
    "\n",
    "Min-Max Scaling:\n",
    "\n",
    "Python\n",
    "scaled_data_minmax = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n",
    "\n",
    "print(scaled_data_minmax)\n",
    "Use code with caution. Learn more\n",
    "Output:\n",
    "\n",
    "[[0.  0. ]\n",
    " [0.5 0.5]\n",
    " [1.  1. ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a3a82-2a21-4314-b44c-95c6c7a9dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "Ans:- \n",
    "* Principal Component Analysis (PCA) Explained\n",
    "PCA (Principal Component Analysis) is a powerful dimensionality reduction technique used in various domains like data analysis, machine learning, and image processing. It aims to transform a high-dimensional dataset into a lower-dimensional one while preserving as much of the original information as possible. This is achieved by identifying the directions (called principal components) of greatest variance in the data and focusing on those instead of using all the original features.\n",
    "\n",
    "Here are some concepts:\n",
    "\n",
    "    > Variance: Indicates the spread of data points around the mean. Higher variance represents greater variability.\n",
    "Principal Components (PCs): Linear combinations of the original features that capture the most variance in the data. \n",
    "The first PC explains the most variance, followed by the second, and so on.\n",
    "Dimensionality Reduction: Reducing the number of features in a dataset by considering only the first few PCs, which often\n",
    "carry the most relevant information.\n",
    "\n",
    "Work: \n",
    "\n",
    "    Centering the data: Subtracts the mean value of each feature from all data points.\n",
    "Calculating the covariance matrix: Measures the linear relationships between pairs of features.\n",
    "Finding eigenvalues and eigenvectors: Eigenvalues represent the variance explained by each PC, and eigenvectors are the \n",
    "directions of the PCs.\n",
    "Projecting data onto PCs: Retains only the first few PCs, which typically explain a significant portion of the total variance.\n",
    "Applications of PCA:\n",
    "Visualization: High-dimensional data can be difficult to visualize directly. PCA allows projecting data onto fewer dimensions\n",
    "for easier visualization techniques like scatter plots or 3D plots.\n",
    "Compression: PCA can be used to compress data by discarding less important features, reducing storage requirements and \n",
    "communication bandwidth.\n",
    "Machine Learning: PCA can improve the performance of machine learning algorithms by reducing the number of features, often \n",
    "leading to faster training and better generalization.\n",
    "Example:\n",
    "Imagine you have a dataset of images represented by hundreds of pixels. Each pixel acts as a feature. While carrying detailed \n",
    "information, this high dimensionality can be computationally expensive for tasks like image classification. PCA can identify \n",
    "the most significant variations in pixel values across images, capturing the essential visual patterns. By retaining only the\n",
    "first few PCs, the images can be efficiently represented in a lower-dimensional space while maintaining key visual \n",
    "characteristics, ultimately benefiting image processing and classification tasks.\n",
    "\n",
    "Advantages of PCA:\n",
    "\n",
    "    Effective dimensionality reduction technique, often preserving significant information.\n",
    "Improves visualization capabilities for high-dimensional data.\n",
    "Enhances performance and efficiency of machine learning algorithms.\n",
    "\n",
    "Disadvantages of PCA:\n",
    "\n",
    "    Depends heavily on linear relationships between features. May not be suitable for non-linear datasets.\n",
    "Loss of information when discarding less important components, requiring careful selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef5514-0661-4111-af19-44f0ebfaaad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "Ans:- \n",
    "PCA and Feature Extraction: A Symbiotic Relationship\n",
    "PCA and Feature Extraction are closely intertwined but not strictly the same thing. Here's how they differ and how PCA can be used for feature extraction:\n",
    "\n",
    "Feature Extraction:\n",
    "\n",
    "Aims to create new features that capture the most relevant information for a specific task, often reducing dimensionality in the process.\n",
    "Can involve various techniques like domain knowledge, filters, or transformations.\n",
    "\n",
    "PCA:\n",
    "\n",
    "Primarily a dimensionality reduction technique, identifying the directions of maximum variance in the data.\n",
    "Can implicitly extract features in the form of the principal components (PCs).\n",
    "Focuses on capturing the most significant variance, which often aligns with relevant information.\n",
    "Using PCA for Feature Extraction:\n",
    "\n",
    "Applying PCA to dataset.\n",
    "\n",
    "Analyze the eigenvalues and eigenvectors:\n",
    "Eigenvalues represent the variance explained by each PC.\n",
    "Choose the PCs with the highest eigenvalues, as they capture the most information.\n",
    "Use the selected PCs as new features: These represent the most informative directions in the original data.\n",
    "\n",
    "Example:\n",
    "\n",
    "Imagine you have a dataset of handwritten digits (like MNIST) with pixel intensities as features.\n",
    "\n",
    "Applying PCA would reveal the directions of greatest variance, capturing variations in stroke patterns, shapes, etc.\n",
    "Keeping the top few PCs would retain these essential features while discarding less informative noise in individual pixels.\n",
    "These PCs could then be used for tasks like digit recognition, potentially performing as well as the original pixels but with significantly fewer features.\n",
    "Key advantages of using PCA for feature extraction:\n",
    "\n",
    "Unsupervised: Doesn't require labeled data, making it useful for exploratory analysis.\n",
    "Computationally efficient: Dimensionality reduction leads to faster processing.\n",
    "Interpretable: PCs can be visualized to understand the captured information.\n",
    "Things to consider:\n",
    "\n",
    "PCA assumes linear relationships. May not be ideal for non-linear data.\n",
    "Choosing the right number of PCs requires careful trade-off between information retention and dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3150dfc-c360-48fa-86be-88f67518607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "Ans:- \n",
    "Applying Min-Max Scaling to Food Delivery Recommendation System\n",
    "Min-Max scaling can be a useful preprocessing step for food delivery recommendation system dataset, especially with features\n",
    "like price, rating, and delivery time, which typically have different scales and units. Here's how you would apply it:\n",
    "\n",
    "1. Identify Features for Scaling:\n",
    "\n",
    "Price: This likely varies significantly across dishes. Scaling would ensure all prices contribute equally to recommendations.\n",
    "Rating: While potentially ranging from 1 to 5, scaling might not be necessary if you're using an appropriate distance-based algorithm. However, consider it if other rating-based features are included.\n",
    "Delivery Time: This again has a wide range (minutes). Scaling will make it comparable to other features.\n",
    "\n",
    "2. Implement Min-Max Scaling:\n",
    "\n",
    "Use a library like scikit-learn in Python with its MinMaxScaler tool.\n",
    "Fit the scaler on the training data to estimate minimum and maximum values for each feature.\n",
    "Transform both training and test data using the fitted scaler. This scales each feature value to the range 0-1.\n",
    "\n",
    "Example (using scikit-learn):\n",
    "\n",
    "Python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data\n",
    "prices = [5, 10, 20]\n",
    "ratings = [3, 4, 4.5]\n",
    "delivery_times = [20, 30, 45]\n",
    "\n",
    "# Create and fit scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit([prices, ratings, delivery_times])\n",
    "\n",
    "# Transform data\n",
    "scaled_prices, scaled_ratings, scaled_delivery_times = scaler.transform([prices, ratings, delivery_times])\n",
    "\n",
    "# Print scaled data\n",
    "print(\"Scaled prices:\", scaled_prices)\n",
    "print(\"Scaled ratings:\", scaled_ratings)\n",
    "print(\"Scaled delivery times:\", scaled_delivery_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ac9996-444a-42a9-bfdf-10297146292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "ANs:- Dimensionality Reduction for Stock Price Prediction with PCA\n",
    "Incorporating multiple features can enhance stock price prediction, but high dimensionality can pose challenges. PCA offers a promising solution by reducing dimensionality while preserving valuable information. Here's how you can employ PCA for your project:\n",
    "\n",
    "1. Feature Selection:\n",
    "\n",
    "* Evaluate the relevance of each feature to stock price prediction based on domain knowledge and statistical analysis. Remove highly correlated or irrelevant features beforehand.\n",
    "* Consider feature engineering to create new features capturing meaningful relationships between existing ones.\n",
    "2. Apply PCA:\n",
    "\n",
    "* Use tools like scikit-learn's PCA for dimension reduction.\n",
    "* Center and standardize the data before applying PCA, as it assumes numerical features with zero mean and unit variance.\n",
    "* Decide on the number of principal components (PCs) to retain:\n",
    "  > Analyze the explained variance ratio (percentage of variance captured by each PC). Aim for a trade-off between information \n",
    "retention and reduced dimensionality.\n",
    "  > Consider scree plots (variance explained vs. PC number) to visualize the diminishing returns of adding more PCs.\n",
    "  > Alternatively, use techniques like eigenvalue-ratio thresholds or cumulative explained variance thresholds to select PCs.\n",
    "3. Use the Transformed Data:\n",
    "\n",
    "> Once you've selected the PCs, transform the entire dataset (training and testing) using the fitted PCA model.\n",
    "> Use the transformed, lower-dimensional dataset as input to your chosen stock price prediction model.\n",
    "Example (using scikit-learn):\n",
    "\n",
    "#HERE IS THE CODE.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample data (replace with your actual features)\n",
    "financial_data = ...\n",
    "market_trends = ...\n",
    "\n",
    "# Combine data (assuming numerical features)\n",
    "data = np.concatenate([financial_data, market_trends], axis=1)\n",
    "\n",
    "# Standardize and center the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Apply PCA with 80% cumulative explained variance\n",
    "pca = PCA(n_components=0.8)\n",
    "pca_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Use pca_data for your prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478f18d4-befe-4585-bdbc-f41fa116710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n",
    "Ans:- \n",
    "\n",
    "1. Find the minimum and maximum values:\n",
    "\n",
    "Minimum value: 1\n",
    "Maximum value: 20\n",
    "Define the desired range:\n",
    "\n",
    "New minimum: -1\n",
    "New maximum: 1\n",
    "\n",
    "Apply the Min-Max scaling formula to each value:\n",
    "scaled_value = (original_value - min_value) * (new_max - new_min) / (max_value - min_value) + new_min\n",
    "\n",
    "Calculate the scaled values:\n",
    "\n",
    "For 1: (1 - 1) * (1 - (-1)) / (20 - 1) - 1 = -1\n",
    "For 5: (5 - 1) * (1 - (-1)) / (20 - 1) - 1 = -0.5789\n",
    "For 10: (10 - 1) * (1 - (-1)) / (20 - 1) - 1 = -0.0526\n",
    "For 15: (15 - 1) * (1 - (-1)) / (20 - 1) - 1 = 0.4737\n",
    "For 20: (20 - 1) * (1 - (-1)) / (20 - 1) - 1 = 1\n",
    "Therefore, the scaled values are: [-1, -0.5789, -0.0526, 0.4737, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cecce0-3463-4431-9495-eddd49214b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "Ans:- \n",
    "Data-driven factors:\n",
    "\n",
    "1.Explained Variance: Analyze the eigenvalues (variance explained by each PC) to see how much information each PC captures. \n",
    "Aim for a trade-off between information retention and dimensionality reduction. For example, retaining PCs that explain 80-90%\n",
    "of the variance might be suitable.\n",
    "2.Scree Plot: Visualize the eigenvalues with a scree plot. Look for an \"elbow\" where the explained variance drops sharply, \n",
    "indicating fewer remaining PCs contribute significantly.\n",
    "3.Correlations: Check for highly correlated features, as PCA captures mainly the variance not covered by existing correlations.\n",
    "\n",
    "Application-specific factors:\n",
    "\n",
    "Modeling task: If you need high accuracy for a specific task (e.g., disease prediction), retaining more PCs might be better, \n",
    "even if it increases dimensionality.\n",
    "Computational constraints: If processing speed is crucial, using fewer PCs reduces computational burden.\n",
    "\n",
    "General :\n",
    "\n",
    "Often, the first few PCs capture a significant portion of the variance.\n",
    "Retaining more PCs improves interpretability, as they represent more fine-grained information.\n",
    "Overfitting is a risk with too many PCs, especially with limited data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
